{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PageRank\n",
    "\n",
    "## A Twitter-Mentioning Graph\n",
    "\n",
    "* The edges are binary and directed. If Bob mentions Alice once, in 10 tweets, or 10 times in one tweet, there is an edge from Bob to Alice, but there is not an edge from Alice to Bob.\n",
    "* If a user mentions herself, ignore it.\n",
    "* Correctly parsing @mentions in a tweet is error-prone. Use the entities field.\n",
    "* Later you will need to implement the PageRank algorithm on the graph you build here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# Here define your function for building the graph by parsing the input file of tweets\n",
    "# Insert as many cells as you want\n",
    "\n",
    "def FileParser(filename):\n",
    "    tweet_graph ={}            #Each (key,value) pair represents an edge from 'key' to 'pair'\n",
    "    \n",
    "    m=0 \n",
    "    with open(filename) as myfname:\n",
    "        m+=1 \n",
    "        for line in myfname:\n",
    "            temp_data= json.loads(line)\n",
    "            id_current_user = temp_data[\"user\"][\"id\"]\n",
    "            if id_current_user not in tweet_graph:\n",
    "                tweet_graph[id_current_user] = set()\n",
    "\n",
    "            \n",
    "            for index in range(len(temp_data[\"entities\"][\"user_mentions\"])):\n",
    "                id_ref_user = temp_data[\"entities\"][\"user_mentions\"][index][\"id\"]\n",
    "                if id_current_user != id_ref_user:               #to avoid self- loops (self- referencing)\n",
    "                    tweet_graph[id_current_user].add(id_ref_user)\n",
    "            \n",
    "            #delete the \"user\" node from graph if it has no out-edges (i.e., it is not referring to anyone)\n",
    "            if len(tweet_graph[id_current_user]) is 0:\n",
    "                del tweet_graph[id_current_user]\n",
    "            \n",
    "\n",
    "    \n",
    "    ##--------------------------------------------------------------------------------------\n",
    "    #step-2 : id_lookup_dict creation: this will be useful for \"pagerank\" calculation later\n",
    "    #id_lookup_dict - contains a simple integer 'value' starting from 0 for each 'userID' as 'key'\n",
    "    # len(id_lookup_dict) = total number of different \"users\"\n",
    "    id_index = 0 \n",
    "    id_lookup_dict={}\n",
    "    for ele in tweet_graph:\n",
    "        if ele not in id_lookup_dict:\n",
    "            id_lookup_dict[ele] = index\n",
    "            index+=1\n",
    "        \n",
    "        for neighbour in tweet_graph[ele]:\n",
    "            if neighbour not in id_lookup_dict:\n",
    "                id_lookup_dict[neighbour] = index\n",
    "                index +=1\n",
    "\n",
    "                \n",
    "    ##-------------------------------------------------------------------------------------\n",
    "    #step-3 : updating \"tweet_graph\" to include all the nodes \n",
    "    for ele in id_lookup_dict:\n",
    "        if ele not in tweet_graph:\n",
    "            tweet_graph[ele]= set()\n",
    "    \n",
    "    return (tweet_graph, id_lookup_dict)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "No. of nodes = 16430 and No. of edges = 24256\n"
     ]
    }
   ],
   "source": [
    "# Call your function to print out the size of the graph, i.e., the number of nodes and edges\n",
    "# How you maintain the graph is totaly up to you\n",
    "# However, if you encounter any memory issues, we recommend you write the graph into a file, and load it later.\n",
    "tweet_graph, id_lookup_dict = FileParser('pagerank.json')\n",
    "tot_edges=0 \n",
    "\n",
    "for node,edge in tweet_graph.iteritems():\n",
    "    tot_edges+= len(edge )\n",
    "\n",
    "print(\"No. of nodes = \"+ str(len(tweet_graph)) + \" and No. of edges = \" +str(tot_edges))    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here add your code to implement a function called PageRanker\n",
    "# Insert as many cells as you want\n",
    "\n",
    "\n",
    "def PageRanker(tweet_graph, id_lookup_dict):  \n",
    "\n",
    "    d = 0.9 \n",
    "    N = len(id_lookup_dict)\n",
    "    page_rank_vector = [(1-d)/float(N)] * N \n",
    "    last_page_rank_vector =None \n",
    "    diff_page_rank_value=  1\n",
    "    \n",
    "    while diff_page_rank_value >= 0.0001 :\n",
    "        \n",
    "        last_page_rank_vector, page_rank_vector = page_rank_vector, [(1-d)/float(N)]*N\n",
    "\n",
    "        for ele in tweet_graph:\n",
    "            ele_length = len(tweet_graph[ele])\n",
    "            \n",
    "            #nodes with no out-edges need not be considered in calculations\n",
    "            if ele_length==0:\n",
    "                continue\n",
    "            ele_PR_value = last_page_rank_vector[id_lookup_dict[ele]]\n",
    "            \n",
    "            #'neighbor' has an in-link from 'ele'. so it will receive a fraction of 'ele's' PR score \n",
    "            for neighbor_id in tweet_graph[ele]: \n",
    "                neighbor_index= id_lookup_dict[neighbor_id]\n",
    "                page_rank_vector[neighbor_index] += float(d*ele_PR_value)/float(ele_length)\n",
    "\n",
    "        #Termination condition error calculation \n",
    "        diff_page_rank_value = 0 \n",
    "        for i in range(len(page_rank_vector)):\n",
    "            diff_page_rank_value += abs(page_rank_vector[i] - last_page_rank_vector[i])\n",
    "        sorted_page_rank_vec_index = sorted(range(len(page_rank_vector)), key=lambda k: page_rank_vector[k], reverse=True)\n",
    "        sorted_last_page_rank_vec_index= sorted(range(len(last_page_rank_vector)), key=lambda k: page_rank_vector[k], reverse=True)\n",
    "        \n",
    "    #normalizing PR score at the end     \n",
    "    tot_score= 0 \n",
    "    for score in page_rank_vector:\n",
    "        tot_score += score\n",
    "        \n",
    "    for i in range(len(page_rank_vector)):\n",
    "        page_rank_vector[i] /=tot_score\n",
    "        \n",
    "    return page_rank_vector\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id= 158314798, score= 0.00771549905257\n",
      "user_id= 181561712, score= 0.00652134413971\n",
      "user_id= 209708391, score= 0.00607439403128\n",
      "user_id= 72064417, score= 0.00553688801417\n",
      "user_id= 105119490, score= 0.00508362381201\n",
      "user_id= 14268057, score= 0.0046790143692\n",
      "user_id= 379961664, score= 0.00442468466829\n",
      "user_id= 391037985, score= 0.00425692488481\n",
      "user_id= 153074065, score= 0.00425357283333\n",
      "user_id= 313525912, score= 0.00409497311133\n"
     ]
    }
   ],
   "source": [
    "# Now let's call your function on the graph you've built. Output the results.\n",
    "\n",
    "tweet_graph, id_lookup_dict = FileParser('pagerank.json')\n",
    "page_rank_vector            = PageRanker(tweet_graph, id_lookup_dict)\n",
    "\n",
    "#Sorting the page rank results and printing top 10 results as per the pagerank score calculated \n",
    "sort_page_rank_vec_index = sorted(range(len(page_rank_vector)), key=lambda k: page_rank_vector[k], reverse=True)\n",
    "for i in sort_page_rank_vec_index[0:10]:\n",
    "    for id,index in id_lookup_dict.iteritems():\n",
    "        if index == i:\n",
    "            print(\"user_id= \" +str(id) + \", score= \"+str(page_rank_vector[index]))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NOTE: Done as a part of CS670 course assignment at Texas A&M University"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
